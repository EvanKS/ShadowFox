{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkc3jeI56jIS"
      },
      "outputs": [],
      "source": [
        "!pip install torch\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fYvC71sn8eoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets scikit-learn matplotlib seaborn bertviz\n"
      ],
      "metadata": {
        "id": "bHxF2KEZ8k-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n"
      ],
      "metadata": {
        "id": "wS0_GOR989YC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n"
      ],
      "metadata": {
        "id": "te__HTif9T8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n"
      ],
      "metadata": {
        "id": "OKbxVFVN9b48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"imdb\")\n"
      ],
      "metadata": {
        "id": "kFopUUQb9t-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shuffled = dataset['train'].shuffle(seed=42)\n",
        "\n",
        "texts = shuffled['text'][:4000]\n",
        "labels = shuffled['label'][:4000]\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train = vectorizer.fit_transform(texts)\n",
        "y_train = labels\n",
        "\n",
        "lr_model = LogisticRegression(max_iter=1000)\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Logistic Regression trained successfully\")"
      ],
      "metadata": {
        "id": "Xh6mtxuO-Zl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "\n",
        "print(\"BERT loaded successfully\")\n"
      ],
      "metadata": {
        "id": "llt_tLCV_Hyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "    \"The movie was not bad at all\",\n",
        "    \"I thought it would be great, but it wasn't\",\n",
        "    \"Absolutely fantastic experience\",\n",
        "    \"Terrible plot but great acting\"\n",
        "]\n",
        "\n",
        "for s in sentences:\n",
        "    inputs = tokenizer(s, return_tensors=\"pt\")\n",
        "    outputs = bert_model(**inputs)\n",
        "    print(s)\n",
        "    print(outputs.logits)\n",
        "    print(\"-----\")\n"
      ],
      "metadata": {
        "id": "1WnkQqoz_R3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "sentences = [\n",
        "    \"The movie was not bad at all\",\n",
        "    \"I thought it would be great, but it wasn't\",\n",
        "    \"Absolutely fantastic experience\",\n",
        "    \"Terrible plot but great acting\"\n",
        "]\n",
        "\n",
        "labels_map = {0: \"Negative\", 1: \"Positive\"}\n",
        "\n",
        "for s in sentences:\n",
        "    inputs = tokenizer(s, return_tensors=\"pt\")\n",
        "    outputs = bert_model(**inputs)\n",
        "\n",
        "    probs = torch.softmax(outputs.logits, dim=1)\n",
        "    pred = torch.argmax(probs).item()\n",
        "\n",
        "    print(f\"Sentence: {s}\")\n",
        "    print(f\"Prediction: {labels_map[pred]}\")\n",
        "    print(f\"Probabilities: {probs.detach().numpy()}\")\n",
        "    print(\"-----\")\n"
      ],
      "metadata": {
        "id": "ocE2ZjZj_ckO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "bert_model = BertForSequenceClassification.from_pretrained(\n",
        "    \"textattack/bert-base-uncased-imdb\",\n",
        "    output_attentions=True # Explicitly enable attention output\n",
        ")\n",
        "\n",
        "print(\"Sentiment BERT loaded\")"
      ],
      "metadata": {
        "id": "8Gc01KI2_82P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "sentences = [\n",
        "    \"The movie was not bad at all\",\n",
        "    \"I thought it would be great, but it wasn't\",\n",
        "    \"Absolutely fantastic experience\",\n",
        "    \"Terrible plot but great acting\",\n",
        "    \"I wouldn't recommend this movie to anyone\",\n",
        "    \"The storyline was boring but the visuals were stunning\",\n",
        "    \"One of the best movies I have ever watched\",\n",
        "    \"It started well but became worse as it went on\"\n",
        "]\n",
        "\n",
        "labels_map = {0: \"Negative\", 1: \"Positive\"}\n",
        "\n",
        "for s in sentences:\n",
        "    inputs = tokenizer(s, return_tensors=\"pt\")\n",
        "    outputs = bert_model(**inputs)\n",
        "\n",
        "    probs = torch.softmax(outputs.logits, dim=1)\n",
        "    pred = torch.argmax(probs).item()\n",
        "\n",
        "    print(f\"Sentence: {s}\")\n",
        "    print(f\"Prediction: {labels_map[pred]}\")\n",
        "    print(f\"Probabilities: {probs.detach().numpy()}\")\n",
        "    print(\"-----\")\n"
      ],
      "metadata": {
        "id": "LrbrZeSgAH97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.backends.cuda.enable_flash_sdp(False)\n",
        "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
        "torch.backends.cuda.enable_math_sdp(True)\n"
      ],
      "metadata": {
        "id": "VRwIk30pAv-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bertviz import head_view\n",
        "\n",
        "text = \"The movie was not bad at all\"\n",
        "inputs = tokenizer(text, return_tensors='pt')\n",
        "\n",
        "outputs = bert_model(**inputs, output_attentions=True)\n",
        "\n",
        "attentions = outputs.attentions\n",
        "print(f\"Attentions: {attentions}\") # Debugging line\n",
        "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "\n",
        "head_view(encoder_attention=attentions, encoder_tokens=tokens)"
      ],
      "metadata": {
        "id": "0gK6RgQlBGRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "models = ['Logistic Regression', 'BERT (Fine-tuned)']\n",
        "accuracy = [0.72, 0.94]\n",
        "\n",
        "plt.bar(models, accuracy)\n",
        "plt.title(\"Model Accuracy Comparison\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "l5VYCZjVBtKR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}